Training Set: {'gurlitt-all-rnr-threads.txt', 'germanwings-crash-all-rnr-threads.txt', 'putinmissing-all-rnr-threads.txt', 'ottawashooting-all-rnr-threads.txt', 'prince-toronto-all-rnr-threads.txt', 'ferguson-all-rnr-threads.txt', 'sydneysiege-all-rnr-threads.txt', 'ebola-essien-all-rnr-threads.txt'}
Iteration 1 Loss: 47.35791778564453
Iteration 2 Loss: 46.3648681640625
Iteration 3 Loss: 45.91510772705078
Iteration 4 Loss: 46.421504974365234
Iteration 5 Loss: 45.439369201660156
Iteration 6 Loss: 45.289337158203125
Iteration 7 Loss: 44.81617736816406
Iteration 8 Loss: 45.999961853027344
Iteration 9 Loss: 45.961692810058594
Iteration 10 Loss: 46.58736801147461
Iteration 11 Loss: 46.6185417175293
Iteration 12 Loss: 44.998592376708984
Iteration 13 Loss: 44.98711013793945
Iteration 14 Loss: 44.71281051635742
Iteration 15 Loss: 45.0296745300293
Iteration 16 Loss: 44.795875549316406
Iteration 17 Loss: 44.71002960205078
Iteration 18 Loss: 44.382869720458984
Iteration 19 Loss: 44.13608932495117
Iteration 20 Loss: 44.735050201416016
Iteration 21 Loss: 45.277400970458984
Iteration 22 Loss: 44.69637680053711
Iteration 23 Loss: 44.406917572021484
Iteration 24 Loss: 44.07272720336914
Iteration 25 Loss: 43.84697341918945
Iteration 26 Loss: 45.34269332885742
Iteration 27 Loss: 44.238582611083984
Iteration 28 Loss: 43.90953063964844
Iteration 29 Loss: 44.22505569458008
Iteration 30 Loss: 44.442161560058594
Iteration 31 Loss: 44.09088897705078
Iteration 32 Loss: 44.283294677734375
Iteration 33 Loss: 43.6991081237793
Iteration 34 Loss: 43.16238784790039
Iteration 35 Loss: 42.76839828491211
Iteration 36 Loss: 42.92076873779297
Iteration 37 Loss: 43.277645111083984
Iteration 38 Loss: 43.2088623046875
Iteration 39 Loss: 43.761817932128906
Iteration 40 Loss: 43.184364318847656
Iteration 41 Loss: 43.60536575317383
Iteration 42 Loss: 42.750614166259766
Iteration 43 Loss: 42.856197357177734
Iteration 44 Loss: 43.241371154785156
Iteration 45 Loss: 42.905033111572266
Iteration 46 Loss: 42.75804138183594
Iteration 47 Loss: 42.39705276489258
Iteration 48 Loss: 42.430023193359375
Iteration 49 Loss: 42.765869140625
Iteration 50 Loss: 42.72513198852539
Iteration 51 Loss: 42.29644775390625
Iteration 52 Loss: 42.669097900390625
Iteration 53 Loss: 41.78205871582031
Iteration 54 Loss: 42.157989501953125
Iteration 55 Loss: 42.258750915527344
Iteration 56 Loss: 42.5393180847168
Iteration 57 Loss: 41.59377670288086
Iteration 58 Loss: 41.52246856689453
Iteration 59 Loss: 42.484371185302734
Iteration 60 Loss: 41.997440338134766
Iteration 61 Loss: 42.45216369628906
Iteration 62 Loss: 42.50508499145508
Iteration 63 Loss: 41.60885238647461
Iteration 64 Loss: 41.47218704223633
Iteration 65 Loss: 41.31063461303711
Iteration 66 Loss: 41.074920654296875
Iteration 67 Loss: 40.84339141845703
Iteration 68 Loss: 40.81618881225586
Iteration 69 Loss: 41.55401611328125
Iteration 70 Loss: 41.51338195800781
Iteration 71 Loss: 40.907684326171875
Iteration 72 Loss: 40.382789611816406
Iteration 73 Loss: 40.235591888427734
Iteration 74 Loss: 40.14665985107422
Iteration 75 Loss: 40.335025787353516
Iteration 76 Loss: 41.02777862548828
Iteration 77 Loss: 41.25181198120117
Iteration 78 Loss: 41.03240203857422
Iteration 79 Loss: 41.01832580566406
Iteration 80 Loss: 41.0421257019043
Iteration 81 Loss: 41.3676643371582
Iteration 82 Loss: 41.44761657714844
Iteration 83 Loss: 40.2791633605957
Iteration 84 Loss: 40.546451568603516
Iteration 85 Loss: 40.61784744262695
Iteration 86 Loss: 40.93893051147461
Iteration 87 Loss: 41.38175582885742
Iteration 88 Loss: 40.446693420410156
Iteration 89 Loss: 40.27375793457031
Iteration 90 Loss: 39.586822509765625
Iteration 91 Loss: 39.53453063964844
Iteration 92 Loss: 39.638916015625
Iteration 93 Loss: 40.0423698425293
Iteration 94 Loss: 39.55046463012695
Iteration 95 Loss: 39.60642623901367
Iteration 96 Loss: 39.14900588989258
Iteration 97 Loss: 39.927886962890625
Iteration 98 Loss: 40.00353240966797
Iteration 99 Loss: 39.18373107910156
Iteration 100 Loss: 39.16368103027344
Training Complete
Now Testing: charliehebdo-all-rnr-threads.txt
charliehebdo-all-rnr-threads.txt accuracy: 0.7508771929824561
charliehebdo-all-rnr-threads.txt f1: 0.5867827156532435
charliehebdo-all-rnr-threads.txt total tested: 1995
Training Set: {'gurlitt-all-rnr-threads.txt', 'germanwings-crash-all-rnr-threads.txt', 'putinmissing-all-rnr-threads.txt', 'ottawashooting-all-rnr-threads.txt', 'prince-toronto-all-rnr-threads.txt', 'charliehebdo-all-rnr-threads.txt', 'ferguson-all-rnr-threads.txt', 'sydneysiege-all-rnr-threads.txt'}
Iteration 1 Loss: 65.60616302490234
Iteration 2 Loss: 65.34060668945312
Iteration 3 Loss: 64.03702545166016
Iteration 4 Loss: 63.2989387512207
Iteration 5 Loss: 65.30213165283203
Iteration 6 Loss: 62.850555419921875
Iteration 7 Loss: 62.09403610229492
Iteration 8 Loss: 61.82321548461914
Iteration 9 Loss: 62.10528564453125
Iteration 10 Loss: 64.51415252685547
Iteration 11 Loss: 62.35265350341797
Iteration 12 Loss: 63.58575439453125
Iteration 13 Loss: 62.94320297241211
Iteration 14 Loss: 64.6786880493164
Iteration 15 Loss: 61.84880828857422
Iteration 16 Loss: 62.0929069519043
Iteration 17 Loss: 62.53123092651367
Iteration 18 Loss: 62.18976974487305
Iteration 19 Loss: 62.01975631713867
Iteration 20 Loss: 63.32782745361328
Iteration 21 Loss: 61.15184783935547
Iteration 22 Loss: 61.72037124633789
Iteration 23 Loss: 61.478939056396484
Iteration 24 Loss: 61.78046798706055
Iteration 25 Loss: 61.92951965332031
Iteration 26 Loss: 62.21952819824219
Iteration 27 Loss: 61.471824645996094
Iteration 28 Loss: 62.23853302001953
Iteration 29 Loss: 60.804718017578125
Iteration 30 Loss: 61.43671417236328
Iteration 31 Loss: 61.66404724121094
Iteration 32 Loss: 61.66686248779297
Iteration 33 Loss: 60.84462356567383
Iteration 34 Loss: 60.219932556152344
Iteration 35 Loss: 60.078582763671875
Iteration 36 Loss: 60.35417938232422
Iteration 37 Loss: 60.51435852050781
Iteration 38 Loss: 61.078330993652344
Iteration 39 Loss: 61.93492126464844
Iteration 40 Loss: 61.2113151550293
Iteration 41 Loss: 61.515159606933594
Iteration 42 Loss: 60.87201690673828
Iteration 43 Loss: 62.25640106201172
Iteration 44 Loss: 60.5445671081543
Iteration 45 Loss: 60.365074157714844
Iteration 46 Loss: 60.69157409667969
Iteration 47 Loss: 61.65477752685547
Iteration 48 Loss: 60.003257751464844
Iteration 49 Loss: 59.526546478271484
Iteration 50 Loss: 60.278751373291016
Iteration 51 Loss: 59.92707824707031
Iteration 52 Loss: 59.9244384765625
Iteration 53 Loss: 60.22885513305664
Iteration 54 Loss: 59.6566276550293
Iteration 55 Loss: 59.717063903808594
Iteration 56 Loss: 60.61487579345703
Iteration 57 Loss: 59.57788848876953
Iteration 58 Loss: 59.81119155883789
Iteration 59 Loss: 60.11729431152344
Iteration 60 Loss: 59.0113410949707
Iteration 61 Loss: 58.48265838623047
Iteration 62 Loss: 58.69472885131836
Iteration 63 Loss: 58.71403503417969
Iteration 64 Loss: 58.73589324951172
Iteration 65 Loss: 57.775123596191406
Iteration 66 Loss: 59.05508041381836
Iteration 67 Loss: 58.95815658569336
Iteration 68 Loss: 57.73212432861328
Iteration 69 Loss: 58.85940170288086
Iteration 70 Loss: 57.83310317993164
Iteration 71 Loss: 57.365760803222656
Iteration 72 Loss: 58.00263977050781
Iteration 73 Loss: 57.651493072509766
Iteration 74 Loss: 57.9632453918457
Iteration 75 Loss: 58.951080322265625
Iteration 76 Loss: 58.43540954589844
Iteration 77 Loss: 57.75432586669922
Iteration 78 Loss: 57.21971893310547
Iteration 79 Loss: 59.06954574584961
Iteration 80 Loss: 57.11509323120117
Iteration 81 Loss: 56.53913497924805
Iteration 82 Loss: 57.48988723754883
Iteration 83 Loss: 57.444759368896484
Iteration 84 Loss: 56.87961959838867
Iteration 85 Loss: 57.052764892578125
Iteration 86 Loss: 58.14232635498047
Iteration 87 Loss: 59.60592269897461
Iteration 88 Loss: 58.081966400146484
Iteration 89 Loss: 57.17521667480469
Iteration 90 Loss: 57.17617416381836
Iteration 91 Loss: 56.455692291259766
Iteration 92 Loss: 56.410125732421875
Iteration 93 Loss: 57.562461853027344
Iteration 94 Loss: 57.196048736572266
Iteration 95 Loss: 57.217872619628906
Iteration 96 Loss: 56.58420181274414
Iteration 97 Loss: 56.50382614135742
Iteration 98 Loss: 56.79283142089844
Iteration 99 Loss: 55.11164474487305
Iteration 100 Loss: 56.55928421020508
Training Complete
Now Testing: ebola-essien-all-rnr-threads.txt
ebola-essien-all-rnr-threads.txt accuracy: 0.7333333333333333
ebola-essien-all-rnr-threads.txt f1: 0.44
ebola-essien-all-rnr-threads.txt total tested: 15
Training Set: {'gurlitt-all-rnr-threads.txt', 'putinmissing-all-rnr-threads.txt', 'ottawashooting-all-rnr-threads.txt', 'prince-toronto-all-rnr-threads.txt', 'charliehebdo-all-rnr-threads.txt', 'ferguson-all-rnr-threads.txt', 'sydneysiege-all-rnr-threads.txt', 'ebola-essien-all-rnr-threads.txt'}
Iteration 1 Loss: 60.80179214477539
Iteration 2 Loss: 61.41849136352539
Iteration 3 Loss: 60.158363342285156
Iteration 4 Loss: 58.60887145996094
Iteration 5 Loss: 59.53557586669922
Iteration 6 Loss: 58.98264694213867
Iteration 7 Loss: 58.933349609375
Iteration 8 Loss: 59.906585693359375
Iteration 9 Loss: 59.588043212890625
Iteration 10 Loss: 58.19182205200195
Iteration 11 Loss: 58.2385368347168
Iteration 12 Loss: 58.03085708618164
Iteration 13 Loss: 58.289344787597656
Iteration 14 Loss: 57.00542068481445
Iteration 15 Loss: 57.761226654052734
Iteration 16 Loss: 56.83980178833008
Iteration 17 Loss: 56.79644012451172
Iteration 18 Loss: 56.616676330566406
Iteration 19 Loss: 56.71086883544922
Iteration 20 Loss: 56.55971145629883
Iteration 21 Loss: 56.79885482788086
Iteration 22 Loss: 57.414005279541016
Iteration 23 Loss: 56.73939514160156
Iteration 24 Loss: 57.379600524902344
Iteration 25 Loss: 56.564361572265625
Iteration 26 Loss: 56.351524353027344
Iteration 27 Loss: 56.432735443115234
Iteration 28 Loss: 55.74083709716797
Iteration 29 Loss: 56.47566604614258
Iteration 30 Loss: 56.38890838623047
Iteration 31 Loss: 56.26028060913086
Iteration 32 Loss: 55.61760711669922
Iteration 33 Loss: 56.660888671875
Iteration 34 Loss: 56.6318359375
Iteration 35 Loss: 56.291282653808594
Iteration 36 Loss: 56.78042221069336
Iteration 37 Loss: 55.09858322143555
Iteration 38 Loss: 55.98086166381836
Iteration 39 Loss: 56.394100189208984
Iteration 40 Loss: 55.80989456176758
Iteration 41 Loss: 55.81999206542969
Iteration 42 Loss: 55.16249465942383
Iteration 43 Loss: 55.676788330078125
Iteration 44 Loss: 55.51618576049805
Iteration 45 Loss: 56.35343551635742
Iteration 46 Loss: 54.8775520324707
Iteration 47 Loss: 55.14653015136719
Iteration 48 Loss: 54.660484313964844
Iteration 49 Loss: 55.58887481689453
Iteration 50 Loss: 54.403350830078125
Iteration 51 Loss: 54.119842529296875
Iteration 52 Loss: 53.57733154296875
Iteration 53 Loss: 56.253631591796875
Iteration 54 Loss: 55.925559997558594
Iteration 55 Loss: 56.69995880126953
Iteration 56 Loss: 54.468509674072266
Iteration 57 Loss: 54.014122009277344
Iteration 58 Loss: 54.06602478027344
Iteration 59 Loss: 54.81508255004883
Iteration 60 Loss: 53.86641311645508
Iteration 61 Loss: 53.9901008605957
Iteration 62 Loss: 53.66255187988281
Iteration 63 Loss: 54.09516143798828
Iteration 64 Loss: 53.53672409057617
Iteration 65 Loss: 53.195167541503906
Iteration 66 Loss: 53.522457122802734
Iteration 67 Loss: 55.41541290283203
Iteration 68 Loss: 54.5084228515625
Iteration 69 Loss: 55.09591293334961
Iteration 70 Loss: 53.89236831665039
Iteration 71 Loss: 53.766109466552734
Iteration 72 Loss: 54.65618133544922
Iteration 73 Loss: 53.9240837097168
Iteration 74 Loss: 53.70327377319336
Iteration 75 Loss: 53.45956802368164
Iteration 76 Loss: 52.69181823730469
Iteration 77 Loss: 52.99917984008789
Iteration 78 Loss: 53.69964599609375
Iteration 79 Loss: 54.23843765258789
Iteration 80 Loss: 53.64994430541992
Iteration 81 Loss: 53.15957260131836
Iteration 82 Loss: 53.53347396850586
Iteration 83 Loss: 53.17690658569336
Iteration 84 Loss: 53.25173568725586
Iteration 85 Loss: 55.196083068847656
Iteration 86 Loss: 55.36418914794922
Iteration 87 Loss: 55.10685729980469
Iteration 88 Loss: 54.672637939453125
Iteration 89 Loss: 53.93454360961914
Iteration 90 Loss: 53.691444396972656
Iteration 91 Loss: 54.64349365234375
Iteration 92 Loss: 54.007423400878906
Iteration 93 Loss: 53.435550689697266
Iteration 94 Loss: 53.43750762939453
Iteration 95 Loss: 52.914031982421875
Iteration 96 Loss: 52.622013092041016
Iteration 97 Loss: 52.52666091918945
Iteration 98 Loss: 52.4226188659668
Iteration 99 Loss: 52.06487274169922
Iteration 100 Loss: 52.39902114868164
Training Complete
Now Testing: germanwings-crash-all-rnr-threads.txt
germanwings-crash-all-rnr-threads.txt accuracy: 0.5148514851485149
germanwings-crash-all-rnr-threads.txt f1: 0.5155443768378613
germanwings-crash-all-rnr-threads.txt total tested: 404
Training Set: {'gurlitt-all-rnr-threads.txt', 'germanwings-crash-all-rnr-threads.txt', 'putinmissing-all-rnr-threads.txt', 'ottawashooting-all-rnr-threads.txt', 'prince-toronto-all-rnr-threads.txt', 'charliehebdo-all-rnr-threads.txt', 'ferguson-all-rnr-threads.txt', 'ebola-essien-all-rnr-threads.txt'}
Iteration 1 Loss: 53.08174514770508
Iteration 2 Loss: 51.02286911010742
Iteration 3 Loss: 51.18978500366211
Iteration 4 Loss: 51.05351257324219
Iteration 5 Loss: 49.273197174072266
Iteration 6 Loss: 49.810951232910156
Iteration 7 Loss: 50.43082046508789
Iteration 8 Loss: 50.760581970214844
Iteration 9 Loss: 51.11900329589844
Iteration 10 Loss: 50.318450927734375
Iteration 11 Loss: 48.4112434387207
Iteration 12 Loss: 48.8998908996582
Iteration 13 Loss: 48.17662811279297
Iteration 14 Loss: 48.53293228149414
Iteration 15 Loss: 50.165916442871094
Iteration 16 Loss: 51.73592758178711
Iteration 17 Loss: 51.05677032470703
Iteration 18 Loss: 49.91457748413086
Iteration 19 Loss: 48.84877014160156
Iteration 20 Loss: 48.451534271240234
Iteration 21 Loss: 47.969459533691406
Iteration 22 Loss: 47.63349533081055
Iteration 23 Loss: 48.5711784362793
Iteration 24 Loss: 48.76909637451172
Iteration 25 Loss: 47.575050354003906
Iteration 26 Loss: 49.053489685058594
Iteration 27 Loss: 48.39902877807617
Iteration 28 Loss: 47.11764907836914
Iteration 29 Loss: 47.40721130371094
Iteration 30 Loss: 48.14186096191406
Iteration 31 Loss: 48.17840576171875
Iteration 32 Loss: 46.70244598388672
Iteration 33 Loss: 46.807838439941406
Iteration 34 Loss: 47.05804443359375
Iteration 35 Loss: 46.997947692871094
Iteration 36 Loss: 47.97306823730469
Iteration 37 Loss: 47.32750701904297
Iteration 38 Loss: 47.655052185058594
Iteration 39 Loss: 46.97578048706055
Iteration 40 Loss: 46.87444305419922
Iteration 41 Loss: 48.68183135986328
Iteration 42 Loss: 46.756290435791016
Iteration 43 Loss: 46.217716217041016
Iteration 44 Loss: 45.92039108276367
Iteration 45 Loss: 46.308738708496094
Iteration 46 Loss: 47.206787109375
Iteration 47 Loss: 48.73044967651367
Iteration 48 Loss: 46.821224212646484
Iteration 49 Loss: 46.71529769897461
Iteration 50 Loss: 46.470123291015625
Iteration 51 Loss: 46.951053619384766
Iteration 52 Loss: 47.28277587890625
Iteration 53 Loss: 46.653465270996094
Iteration 54 Loss: 45.310203552246094
Iteration 55 Loss: 45.955265045166016
Iteration 56 Loss: 45.887264251708984
Iteration 57 Loss: 47.77039337158203
Iteration 58 Loss: 49.164249420166016
Iteration 59 Loss: 46.124595642089844
Iteration 60 Loss: 46.60491180419922
Iteration 61 Loss: 45.44562911987305
Iteration 62 Loss: 45.52309799194336
Iteration 63 Loss: 45.98445510864258
Iteration 64 Loss: 45.322845458984375
Iteration 65 Loss: 46.29496765136719
Iteration 66 Loss: 46.18812942504883
Iteration 67 Loss: 45.48905563354492
Iteration 68 Loss: 44.64997100830078
Iteration 69 Loss: 44.9962272644043
Iteration 70 Loss: 46.40595626831055
Iteration 71 Loss: 46.68818664550781
Iteration 72 Loss: 45.686859130859375
Iteration 73 Loss: 46.52701950073242
Iteration 74 Loss: 45.69687271118164
Iteration 75 Loss: 45.61543273925781
Iteration 76 Loss: 46.00727844238281
Iteration 77 Loss: 45.53725814819336
Iteration 78 Loss: 44.29215621948242
Iteration 79 Loss: 44.187313079833984
Iteration 80 Loss: 44.328311920166016
Iteration 81 Loss: 45.365638732910156
Iteration 82 Loss: 46.61947250366211
Iteration 83 Loss: 45.38715744018555
Iteration 84 Loss: 46.380897521972656
Iteration 85 Loss: 45.27430725097656
Iteration 86 Loss: 43.9187126159668
Iteration 87 Loss: 43.90414047241211
Iteration 88 Loss: 45.51481246948242
Iteration 89 Loss: 44.700538635253906
Iteration 90 Loss: 44.086421966552734
Iteration 91 Loss: 44.284305572509766
Iteration 92 Loss: 45.225608825683594
Iteration 93 Loss: 44.853721618652344
Iteration 94 Loss: 44.305091857910156
Iteration 95 Loss: 44.018898010253906
Iteration 96 Loss: 43.819923400878906
Iteration 97 Loss: 43.76326370239258
Iteration 98 Loss: 44.66047286987305
Iteration 99 Loss: 45.230064392089844
Iteration 100 Loss: 47.30720138549805
Training Complete
Now Testing: sydneysiege-all-rnr-threads.txt
sydneysiege-all-rnr-threads.txt accuracy: 0.5994876174210076
sydneysiege-all-rnr-threads.txt f1: 0.5975113860556468
sydneysiege-all-rnr-threads.txt total tested: 1171
Training Set: {'germanwings-crash-all-rnr-threads.txt', 'putinmissing-all-rnr-threads.txt', 'ottawashooting-all-rnr-threads.txt', 'prince-toronto-all-rnr-threads.txt', 'charliehebdo-all-rnr-threads.txt', 'ferguson-all-rnr-threads.txt', 'sydneysiege-all-rnr-threads.txt', 'ebola-essien-all-rnr-threads.txt'}
Iteration 1 Loss: 65.62702941894531
Iteration 2 Loss: 65.40188598632812
Iteration 3 Loss: 64.2437744140625
Iteration 4 Loss: 65.4287338256836
Iteration 5 Loss: 64.39924621582031
Iteration 6 Loss: 64.11833953857422
Iteration 7 Loss: 65.24858093261719
Iteration 8 Loss: 65.06251525878906
Iteration 9 Loss: 64.66243743896484
Iteration 10 Loss: 63.380950927734375
Iteration 11 Loss: 63.11446762084961
Iteration 12 Loss: 62.908424377441406
Iteration 13 Loss: 64.13419342041016
Iteration 14 Loss: 62.84836959838867
Iteration 15 Loss: 62.60284423828125
Iteration 16 Loss: 62.7616081237793
Iteration 17 Loss: 63.64147186279297
Iteration 18 Loss: 62.40144729614258
Iteration 19 Loss: 62.210140228271484
Iteration 20 Loss: 61.97628402709961
Iteration 21 Loss: 61.071781158447266
Iteration 22 Loss: 63.015628814697266
Iteration 23 Loss: 62.2021369934082
Iteration 24 Loss: 62.00347900390625
Iteration 25 Loss: 62.197113037109375
Iteration 26 Loss: 62.31222915649414
Iteration 27 Loss: 61.3990364074707
Iteration 28 Loss: 61.021156311035156
Iteration 29 Loss: 60.920562744140625
Iteration 30 Loss: 60.6447639465332
Iteration 31 Loss: 61.173828125
Iteration 32 Loss: 61.76436996459961
Iteration 33 Loss: 60.76394271850586
Iteration 34 Loss: 61.066532135009766
Iteration 35 Loss: 62.09812545776367
Iteration 36 Loss: 60.56058883666992
Iteration 37 Loss: 60.93648910522461
Iteration 38 Loss: 60.81118392944336
Iteration 39 Loss: 60.72317123413086
Iteration 40 Loss: 60.10173797607422
Iteration 41 Loss: 61.12140655517578
Iteration 42 Loss: 60.142330169677734
Iteration 43 Loss: 60.224037170410156
Iteration 44 Loss: 59.98782730102539
Iteration 45 Loss: 60.5588493347168
Iteration 46 Loss: 59.81376647949219
Iteration 47 Loss: 59.87348937988281
Iteration 48 Loss: 60.816917419433594
Iteration 49 Loss: 61.43634033203125
Iteration 50 Loss: 59.748416900634766
Iteration 51 Loss: 60.1767463684082
Iteration 52 Loss: 60.04344177246094
Iteration 53 Loss: 60.34841537475586
Iteration 54 Loss: 58.807106018066406
Iteration 55 Loss: 59.76380920410156
Iteration 56 Loss: 59.28313064575195
Iteration 57 Loss: 59.5517578125
Iteration 58 Loss: 58.567508697509766
Iteration 59 Loss: 58.83796691894531
Iteration 60 Loss: 59.13264083862305
Iteration 61 Loss: 59.71833419799805
Iteration 62 Loss: 61.85316848754883
Iteration 63 Loss: 61.73705291748047
Iteration 64 Loss: 58.85334396362305
Iteration 65 Loss: 59.44629669189453
Iteration 66 Loss: 60.1815071105957
Iteration 67 Loss: 59.18585205078125
Iteration 68 Loss: 58.62083053588867
Iteration 69 Loss: 58.7279167175293
Iteration 70 Loss: 58.31970977783203
Iteration 71 Loss: 57.40468215942383
Iteration 72 Loss: 58.78966522216797
Iteration 73 Loss: 60.12500762939453
Iteration 74 Loss: 59.60231018066406
Iteration 75 Loss: 58.89727783203125
Iteration 76 Loss: 58.26570129394531
Iteration 77 Loss: 58.99916076660156
Iteration 78 Loss: 58.16659164428711
Iteration 79 Loss: 57.93462371826172
Iteration 80 Loss: 57.837162017822266
Iteration 81 Loss: 57.29432678222656
Iteration 82 Loss: 57.706199645996094
Iteration 83 Loss: 57.99834060668945
Iteration 84 Loss: 57.43281173706055
Iteration 85 Loss: 58.747703552246094
Iteration 86 Loss: 57.38203811645508
Iteration 87 Loss: 58.428565979003906
Iteration 88 Loss: 56.99477005004883
Iteration 89 Loss: 56.979671478271484
Iteration 90 Loss: 56.99618148803711
Iteration 91 Loss: 56.87222671508789
Iteration 92 Loss: 56.999107360839844
Iteration 93 Loss: 57.887359619140625
Iteration 94 Loss: 56.808921813964844
Iteration 95 Loss: 59.25228500366211
Iteration 96 Loss: 57.90652847290039
Iteration 97 Loss: 57.38747787475586
Iteration 98 Loss: 56.48625946044922
Iteration 99 Loss: 57.265411376953125
Iteration 100 Loss: 57.01340103149414
Training Complete
Now Testing: gurlitt-all-rnr-threads.txt
gurlitt-all-rnr-threads.txt accuracy: 0.3125
gurlitt-all-rnr-threads.txt f1: 0.3055555555555555
gurlitt-all-rnr-threads.txt total tested: 16
Training Set: {'gurlitt-all-rnr-threads.txt', 'germanwings-crash-all-rnr-threads.txt', 'putinmissing-all-rnr-threads.txt', 'ottawashooting-all-rnr-threads.txt', 'charliehebdo-all-rnr-threads.txt', 'ferguson-all-rnr-threads.txt', 'sydneysiege-all-rnr-threads.txt', 'ebola-essien-all-rnr-threads.txt'}
Iteration 1 Loss: 65.15955352783203
Iteration 2 Loss: 65.61737823486328
Iteration 3 Loss: 64.87962341308594
Iteration 4 Loss: 62.41484832763672
Iteration 5 Loss: 62.64820861816406
Iteration 6 Loss: 61.87940979003906
Iteration 7 Loss: 61.231483459472656
Iteration 8 Loss: 61.669944763183594
Iteration 9 Loss: 63.07841873168945
Iteration 10 Loss: 60.74052810668945
Iteration 11 Loss: 61.00497817993164
Iteration 12 Loss: 59.717525482177734
Iteration 13 Loss: 59.312278747558594
Iteration 14 Loss: 62.14567565917969
Iteration 15 Loss: 60.935176849365234
Iteration 16 Loss: 60.39337921142578
Iteration 17 Loss: 61.026275634765625
Iteration 18 Loss: 60.824642181396484
Iteration 19 Loss: 58.98883056640625
Iteration 20 Loss: 60.14922332763672
Iteration 21 Loss: 58.761844635009766
Iteration 22 Loss: 59.2381477355957
Iteration 23 Loss: 60.442935943603516
Iteration 24 Loss: 58.52677536010742
Iteration 25 Loss: 59.01893615722656
Iteration 26 Loss: 58.842979431152344
Iteration 27 Loss: 58.15345764160156
Iteration 28 Loss: 59.61838912963867
Iteration 29 Loss: 61.04874801635742
Iteration 30 Loss: 59.8373908996582
Iteration 31 Loss: 59.56767654418945
Iteration 32 Loss: 58.5165901184082
Iteration 33 Loss: 57.80592346191406
Iteration 34 Loss: 56.83579635620117
Iteration 35 Loss: 57.875091552734375
Iteration 36 Loss: 58.93196105957031
Iteration 37 Loss: 57.60683059692383
Iteration 38 Loss: 58.53693771362305
Iteration 39 Loss: 57.51799774169922
Iteration 40 Loss: 57.28561782836914
Iteration 41 Loss: 56.70465850830078
Iteration 42 Loss: 56.56437301635742
Iteration 43 Loss: 58.2656135559082
Iteration 44 Loss: 57.34685516357422
Iteration 45 Loss: 57.39519119262695
Iteration 46 Loss: 56.83313751220703
Iteration 47 Loss: 56.81004333496094
Iteration 48 Loss: 57.7741584777832
Iteration 49 Loss: 56.26407241821289
Iteration 50 Loss: 57.153446197509766
Iteration 51 Loss: 55.597312927246094
Iteration 52 Loss: 55.65932846069336
Iteration 53 Loss: 55.967079162597656
Iteration 54 Loss: 55.85678482055664
Iteration 55 Loss: 56.03582000732422
Iteration 56 Loss: 55.774532318115234
Iteration 57 Loss: 56.53662872314453
Iteration 58 Loss: 56.94049072265625
Iteration 59 Loss: 57.85658645629883
Iteration 60 Loss: 56.572898864746094
Iteration 61 Loss: 54.842979431152344
Iteration 62 Loss: 54.86418914794922
Iteration 63 Loss: 54.836605072021484
Iteration 64 Loss: 54.615901947021484
Iteration 65 Loss: 55.19963455200195
Iteration 66 Loss: 56.44728088378906
Iteration 67 Loss: 56.60252380371094
Iteration 68 Loss: 56.076141357421875
Iteration 69 Loss: 57.81191635131836
Iteration 70 Loss: 54.808624267578125
Iteration 71 Loss: 54.587806701660156
Iteration 72 Loss: 54.32923126220703
Iteration 73 Loss: 54.87295150756836
Iteration 74 Loss: 54.790428161621094
Iteration 75 Loss: 55.35227584838867
Iteration 76 Loss: 54.48091125488281
Iteration 77 Loss: 53.401790618896484
Iteration 78 Loss: 54.812923431396484
Iteration 79 Loss: 55.9703254699707
Iteration 80 Loss: 53.847808837890625
Iteration 81 Loss: 54.797271728515625
Iteration 82 Loss: 55.427616119384766
Iteration 83 Loss: 57.96364974975586
Iteration 84 Loss: 56.70304489135742
Iteration 85 Loss: 57.088462829589844
Iteration 86 Loss: 58.08513641357422
Iteration 87 Loss: 56.099853515625
Iteration 88 Loss: 56.33458709716797
Iteration 89 Loss: 56.86371612548828
Iteration 90 Loss: 56.65365982055664
Iteration 91 Loss: 56.66169738769531
Iteration 92 Loss: 57.400516510009766
Iteration 93 Loss: 56.641910552978516
Iteration 94 Loss: 56.704322814941406
Iteration 95 Loss: 54.878448486328125
Iteration 96 Loss: 56.232444763183594
Iteration 97 Loss: 54.91971206665039
Iteration 98 Loss: 56.71135330200195
Iteration 99 Loss: 56.85246658325195
Iteration 100 Loss: 56.645957946777344
Training Complete
Now Testing: prince-toronto-all-rnr-threads.txt
prince-toronto-all-rnr-threads.txt accuracy: 0.1865671641791045
prince-toronto-all-rnr-threads.txt f1: 0.1672077922077922
prince-toronto-all-rnr-threads.txt total tested: 134
Training Set: {'gurlitt-all-rnr-threads.txt', 'germanwings-crash-all-rnr-threads.txt', 'putinmissing-all-rnr-threads.txt', 'prince-toronto-all-rnr-threads.txt', 'charliehebdo-all-rnr-threads.txt', 'ferguson-all-rnr-threads.txt', 'sydneysiege-all-rnr-threads.txt', 'ebola-essien-all-rnr-threads.txt'}
Iteration 1 Loss: 55.37526321411133
Iteration 2 Loss: 54.59286880493164
Iteration 3 Loss: 54.53818893432617
Iteration 4 Loss: 53.85302734375
Iteration 5 Loss: 53.748844146728516
Iteration 6 Loss: 54.915367126464844
Iteration 7 Loss: 55.008888244628906
Iteration 8 Loss: 53.92803192138672
Iteration 9 Loss: 54.0007438659668
Iteration 10 Loss: 53.038021087646484
Iteration 11 Loss: 53.28485107421875
Iteration 12 Loss: 52.667198181152344
Iteration 13 Loss: 52.61605453491211
Iteration 14 Loss: 53.020362854003906
Iteration 15 Loss: 53.828041076660156
Iteration 16 Loss: 52.79359817504883
Iteration 17 Loss: 53.00303649902344
Iteration 18 Loss: 52.011470794677734
Iteration 19 Loss: 52.333152770996094
Iteration 20 Loss: 53.05851745605469
Iteration 21 Loss: 52.58116149902344
Iteration 22 Loss: 51.666954040527344
Iteration 23 Loss: 52.11260223388672
Iteration 24 Loss: 52.02518844604492
Iteration 25 Loss: 51.59669494628906
Iteration 26 Loss: 52.184871673583984
Iteration 27 Loss: 51.62161636352539
Iteration 28 Loss: 51.798004150390625
Iteration 29 Loss: 51.75645446777344
Iteration 30 Loss: 52.135868072509766
Iteration 31 Loss: 52.01906967163086
Iteration 32 Loss: 52.45616149902344
Iteration 33 Loss: 50.969234466552734
Iteration 34 Loss: 50.64459991455078
Iteration 35 Loss: 50.302703857421875
Iteration 36 Loss: 49.93561553955078
Iteration 37 Loss: 50.196937561035156
Iteration 38 Loss: 50.34349822998047
Iteration 39 Loss: 49.729888916015625
Iteration 40 Loss: 50.0203971862793
Iteration 41 Loss: 50.34235763549805
Iteration 42 Loss: 50.416446685791016
Iteration 43 Loss: 50.66029739379883
Iteration 44 Loss: 50.675132751464844
Iteration 45 Loss: 50.01740646362305
Iteration 46 Loss: 49.79569625854492
Iteration 47 Loss: 50.07236099243164
Iteration 48 Loss: 51.24138259887695
Iteration 49 Loss: 50.436180114746094
Iteration 50 Loss: 50.94478988647461
Iteration 51 Loss: 51.85000228881836
Iteration 52 Loss: 51.23582458496094
Iteration 53 Loss: 49.8121223449707
Iteration 54 Loss: 49.23548889160156
Iteration 55 Loss: 50.20187759399414
Iteration 56 Loss: 49.847286224365234
Iteration 57 Loss: 49.31916046142578
Iteration 58 Loss: 49.179222106933594
Iteration 59 Loss: 49.33832550048828
Iteration 60 Loss: 50.33515548706055
Iteration 61 Loss: 51.208160400390625
Iteration 62 Loss: 50.74476623535156
Iteration 63 Loss: 50.15971374511719
Iteration 64 Loss: 50.042198181152344
Iteration 65 Loss: 49.05952072143555
Iteration 66 Loss: 49.522193908691406
Iteration 67 Loss: 49.48212814331055
Iteration 68 Loss: 50.693458557128906
Iteration 69 Loss: 49.21223831176758
Iteration 70 Loss: 50.06830596923828
Iteration 71 Loss: 50.40929412841797
Iteration 72 Loss: 49.27143096923828
Iteration 73 Loss: 48.84239196777344
Iteration 74 Loss: 49.68361282348633
Iteration 75 Loss: 48.65541458129883
Iteration 76 Loss: 49.4783821105957
Iteration 77 Loss: 48.420772552490234
Iteration 78 Loss: 48.24369430541992
Iteration 79 Loss: 48.3311653137207
Iteration 80 Loss: 49.25232696533203
Iteration 81 Loss: 48.373016357421875
Iteration 82 Loss: 48.66334915161133
Iteration 83 Loss: 48.165557861328125
Iteration 84 Loss: 48.1409912109375
Iteration 85 Loss: 48.7553825378418
Iteration 86 Loss: 48.244972229003906
Iteration 87 Loss: 47.88442611694336
Iteration 88 Loss: 48.08892822265625
Iteration 89 Loss: 48.1645393371582
Iteration 90 Loss: 48.28793716430664
Iteration 91 Loss: 48.222591400146484
Iteration 92 Loss: 48.10740661621094
Iteration 93 Loss: 47.28997802734375
Iteration 94 Loss: 47.74017333984375
Iteration 95 Loss: 47.49040985107422
Iteration 96 Loss: 47.35316467285156
Iteration 97 Loss: 47.47690200805664
Iteration 98 Loss: 48.31742477416992
Iteration 99 Loss: 47.1644401550293
Iteration 100 Loss: 48.172542572021484
Training Complete
Now Testing: ottawashooting-all-rnr-threads.txt
ottawashooting-all-rnr-threads.txt accuracy: 0.5497076023391813
ottawashooting-all-rnr-threads.txt f1: 0.550348821903383
ottawashooting-all-rnr-threads.txt total tested: 855
Training Set: {'gurlitt-all-rnr-threads.txt', 'germanwings-crash-all-rnr-threads.txt', 'ottawashooting-all-rnr-threads.txt', 'prince-toronto-all-rnr-threads.txt', 'charliehebdo-all-rnr-threads.txt', 'ferguson-all-rnr-threads.txt', 'sydneysiege-all-rnr-threads.txt', 'ebola-essien-all-rnr-threads.txt'}
Iteration 1 Loss: 65.38987731933594
Iteration 2 Loss: 64.47789764404297
Iteration 3 Loss: 63.864479064941406
Iteration 4 Loss: 64.5385513305664
Iteration 5 Loss: 62.95343017578125
Iteration 6 Loss: 62.98240280151367
Iteration 7 Loss: 62.35478591918945
Iteration 8 Loss: 63.83686065673828
Iteration 9 Loss: 63.26594161987305
Iteration 10 Loss: 62.051300048828125
Iteration 11 Loss: 61.52332305908203
Iteration 12 Loss: 60.616329193115234
Iteration 13 Loss: 60.78843688964844
Iteration 14 Loss: 60.65771484375
Iteration 15 Loss: 60.003578186035156
Iteration 16 Loss: 60.93217849731445
Iteration 17 Loss: 60.1478385925293
Iteration 18 Loss: 59.63551712036133
Iteration 19 Loss: 59.44375991821289
Iteration 20 Loss: 61.06758499145508
Iteration 21 Loss: 60.54751968383789
Iteration 22 Loss: 59.96638488769531
Iteration 23 Loss: 58.596683502197266
Iteration 24 Loss: 58.964290618896484
Iteration 25 Loss: 60.19744873046875
Iteration 26 Loss: 60.069095611572266
Iteration 27 Loss: 59.022560119628906
Iteration 28 Loss: 58.32223129272461
Iteration 29 Loss: 58.591644287109375
Iteration 30 Loss: 60.39112091064453
Iteration 31 Loss: 58.73856735229492
Iteration 32 Loss: 59.268680572509766
Iteration 33 Loss: 59.390872955322266
Iteration 34 Loss: 59.29095458984375
Iteration 35 Loss: 57.76936721801758
Iteration 36 Loss: 57.30897521972656
Iteration 37 Loss: 58.376956939697266
Iteration 38 Loss: 59.14562225341797
Iteration 39 Loss: 61.02600860595703
Iteration 40 Loss: 61.508148193359375
Iteration 41 Loss: 58.926124572753906
Iteration 42 Loss: 58.9222297668457
Iteration 43 Loss: 60.262237548828125
Iteration 44 Loss: 62.42555236816406
Iteration 45 Loss: 58.07345199584961
Iteration 46 Loss: 57.60126876831055
Iteration 47 Loss: 57.4147834777832
Iteration 48 Loss: 57.863155364990234
Iteration 49 Loss: 57.65838623046875
Iteration 50 Loss: 57.99525451660156
Iteration 51 Loss: 59.464927673339844
Iteration 52 Loss: 57.7926025390625
Iteration 53 Loss: 58.01622009277344
Iteration 54 Loss: 60.455684661865234
Iteration 55 Loss: 58.65967559814453
Iteration 56 Loss: 57.659427642822266
Iteration 57 Loss: 57.57051086425781
Iteration 58 Loss: 56.99345397949219
Iteration 59 Loss: 57.558597564697266
Iteration 60 Loss: 57.203102111816406
Iteration 61 Loss: 56.920467376708984
Iteration 62 Loss: 56.67107391357422
Iteration 63 Loss: 56.76903533935547
Iteration 64 Loss: 58.214271545410156
Iteration 65 Loss: 62.352169036865234
Iteration 66 Loss: 61.57094955444336
Iteration 67 Loss: 61.68584060668945
Iteration 68 Loss: 62.087799072265625
Iteration 69 Loss: 60.658546447753906
Iteration 70 Loss: 60.16081619262695
Iteration 71 Loss: 59.47663879394531
Iteration 72 Loss: 58.81682205200195
Iteration 73 Loss: 59.824459075927734
Iteration 74 Loss: 59.304595947265625
Iteration 75 Loss: 57.92059326171875
Iteration 76 Loss: 59.25230026245117
Iteration 77 Loss: 58.886497497558594
Iteration 78 Loss: 59.33511734008789
Iteration 79 Loss: 58.78230667114258
Iteration 80 Loss: 58.656166076660156
Iteration 81 Loss: 58.012901306152344
Iteration 82 Loss: 58.3421516418457
Iteration 83 Loss: 57.31016540527344
Iteration 84 Loss: 58.09382629394531
Iteration 85 Loss: 57.75245666503906
Iteration 86 Loss: 58.98833084106445
Iteration 87 Loss: 58.69189453125
Iteration 88 Loss: 58.53878402709961
Iteration 89 Loss: 57.5861930847168
Iteration 90 Loss: 59.025665283203125
Iteration 91 Loss: 59.331485748291016
Iteration 92 Loss: 58.8028450012207
Iteration 93 Loss: 57.930904388427734
Iteration 94 Loss: 57.57185745239258
Iteration 95 Loss: 57.80206298828125
Iteration 96 Loss: 57.31237030029297
Iteration 97 Loss: 57.5245246887207
Iteration 98 Loss: 59.016693115234375
Iteration 99 Loss: 57.162841796875
Iteration 100 Loss: 57.95342254638672
Training Complete
Now Testing: putinmissing-all-rnr-threads.txt
putinmissing-all-rnr-threads.txt accuracy: 0.4485294117647059
putinmissing-all-rnr-threads.txt f1: 0.44672131147540983
putinmissing-all-rnr-threads.txt total tested: 136
Training Set: {'gurlitt-all-rnr-threads.txt', 'germanwings-crash-all-rnr-threads.txt', 'putinmissing-all-rnr-threads.txt', 'ottawashooting-all-rnr-threads.txt', 'prince-toronto-all-rnr-threads.txt', 'charliehebdo-all-rnr-threads.txt', 'sydneysiege-all-rnr-threads.txt', 'ebola-essien-all-rnr-threads.txt'}
Iteration 1 Loss: 56.57408142089844
Iteration 2 Loss: 55.50265884399414
Iteration 3 Loss: 55.71208572387695
Iteration 4 Loss: 54.80452346801758
Iteration 5 Loss: 52.590614318847656
Iteration 6 Loss: 52.98598098754883
Iteration 7 Loss: 52.93309020996094
Iteration 8 Loss: 52.92313003540039
Iteration 9 Loss: 51.85691833496094
Iteration 10 Loss: 52.82075119018555
Iteration 11 Loss: 52.357177734375
Iteration 12 Loss: 51.635154724121094
Iteration 13 Loss: 51.61289596557617
Iteration 14 Loss: 51.24165725708008
Iteration 15 Loss: 51.718162536621094
Iteration 16 Loss: 50.53953552246094
Iteration 17 Loss: 50.98448944091797
Iteration 18 Loss: 51.430721282958984
Iteration 19 Loss: 50.56608581542969
Iteration 20 Loss: 50.13526153564453
Iteration 21 Loss: 50.239646911621094
Iteration 22 Loss: 51.59486770629883
Iteration 23 Loss: 50.32411575317383
Iteration 24 Loss: 50.11452865600586
Iteration 25 Loss: 51.92527389526367
Iteration 26 Loss: 50.560970306396484
Iteration 27 Loss: 50.0113525390625
Iteration 28 Loss: 49.88481903076172
Iteration 29 Loss: 50.17746353149414
Iteration 30 Loss: 50.04664993286133
Iteration 31 Loss: 49.475887298583984
Iteration 32 Loss: 50.00535583496094
Iteration 33 Loss: 50.505088806152344
Iteration 34 Loss: 49.6004753112793
Iteration 35 Loss: 49.59101104736328
Iteration 36 Loss: 49.08744812011719
Iteration 37 Loss: 50.23554611206055
Iteration 38 Loss: 49.21900939941406
Iteration 39 Loss: 49.26658248901367
Iteration 40 Loss: 50.089412689208984
Iteration 41 Loss: 50.038230895996094
Iteration 42 Loss: 49.217018127441406
Iteration 43 Loss: 48.7795295715332
Iteration 44 Loss: 49.40398406982422
Iteration 45 Loss: 49.70501708984375
Iteration 46 Loss: 49.37533187866211
Iteration 47 Loss: 49.1820182800293
Iteration 48 Loss: 48.37865447998047
Iteration 49 Loss: 49.08537673950195
Iteration 50 Loss: 48.628414154052734
Iteration 51 Loss: 48.25271987915039
Iteration 52 Loss: 49.30596923828125
Iteration 53 Loss: 48.20454025268555
Iteration 54 Loss: 48.68244934082031
Iteration 55 Loss: 48.383174896240234
Iteration 56 Loss: 48.67958068847656
Iteration 57 Loss: 48.836055755615234
Iteration 58 Loss: 48.80235290527344
Iteration 59 Loss: 48.4404411315918
Iteration 60 Loss: 48.69108963012695
Iteration 61 Loss: 47.344215393066406
Iteration 62 Loss: 47.31392288208008
Iteration 63 Loss: 47.51023483276367
Iteration 64 Loss: 47.22328567504883
Iteration 65 Loss: 47.36715316772461
Iteration 66 Loss: 47.902130126953125
Iteration 67 Loss: 47.01683044433594
Iteration 68 Loss: 47.12333297729492
Iteration 69 Loss: 47.37065505981445
Iteration 70 Loss: 47.11997604370117
Iteration 71 Loss: 46.391658782958984
Iteration 72 Loss: 46.397769927978516
Iteration 73 Loss: 46.62617111206055
Iteration 74 Loss: 46.64836502075195
Iteration 75 Loss: 45.991451263427734
Iteration 76 Loss: 46.580509185791016
Iteration 77 Loss: 46.341796875
Iteration 78 Loss: 46.081016540527344
Iteration 79 Loss: 46.101200103759766
Iteration 80 Loss: 45.50750732421875
Iteration 81 Loss: 45.30337905883789
Iteration 82 Loss: 46.07080078125
Iteration 83 Loss: 46.03094482421875
Iteration 84 Loss: 46.43844985961914
Iteration 85 Loss: 46.747703552246094
Iteration 86 Loss: 46.7599983215332
Iteration 87 Loss: 46.8303108215332
Iteration 88 Loss: 46.11808395385742
Iteration 89 Loss: 45.939144134521484
Iteration 90 Loss: 46.720699310302734
Iteration 91 Loss: 46.3739128112793
Iteration 92 Loss: 46.40513610839844
Iteration 93 Loss: 45.415164947509766
Iteration 94 Loss: 45.73787307739258
Iteration 95 Loss: 46.793983459472656
Iteration 96 Loss: 46.08577346801758
Iteration 97 Loss: 46.475101470947266
Iteration 98 Loss: 46.29914093017578
Iteration 99 Loss: 45.197696685791016
Iteration 100 Loss: 44.61540985107422
Training Complete
Now Testing: ferguson-all-rnr-threads.txt
ferguson-all-rnr-threads.txt accuracy: 0.39345887016848363
ferguson-all-rnr-threads.txt f1: 0.39347612215012423
ferguson-all-rnr-threads.txt total tested: 1009
karthik@karthik-raavi:~/Verified-Summarization-master_TreeLSTM$ 

